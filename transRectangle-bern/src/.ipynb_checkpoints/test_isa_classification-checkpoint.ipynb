{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=UTF-8\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import os\n",
    "mix = False\n",
    "dim = 100\n",
    "delta_ins, delta_sub = 0, 0\n",
    "dataSet = \"YAGO1\"\n",
    "\n",
    "def prepare(valid, mix, dataSet, ins_wrong, ins_right, sub_wrong, sub_right):\n",
    "    print('-----prepare data -----')\n",
    "    if valid == 'valid':\n",
    "        if mix:\n",
    "            fin = open(\"../../data/\" + dataSet + \"/M-Valid/instanceOf2id_negative.txt\", 'r', encoding='utf-8')\n",
    "            fin_right = open(\"../../data/\" + dataSet + \"/M-Valid/instanceOf2id_positive.txt\", 'r', encoding='utf-8')\n",
    "        else:\n",
    "            fin = open(\"../../data/\" + dataSet + \"/Valid/instanceOf2id_negative.txt\", 'r', encoding='utf-8')\n",
    "            fin_right = open(\"../../data/\" + dataSet + \"/Valid/instanceOf2id_positive.txt\", 'r', encoding='utf-8')\n",
    "    else:\n",
    "        if mix:\n",
    "            fin = open(\"../../data/\" + dataSet + \"/M-Test/instanceOf2id_negative.txt\", 'r', encoding='utf-8')\n",
    "            fin_right = open(\"../../data/\" + dataSet + \"/M-Test/instanceOf2id_positive.txt\", 'r', encoding='utf-8')\n",
    "        else:\n",
    "            fin = open(\"../../data/\" + dataSet + \"/Test/instanceOf2id_negative.txt\", 'r', encoding='utf-8')\n",
    "            fin_right = open(\"../../data/\" + dataSet + \"/Test/instanceOf2id_positive.txt\", 'r', encoding='utf-8')\n",
    "\n",
    "    ins_test_num = int(fin.readline().strip('\\n'))\n",
    "    ins_test_num = int(fin_right.readline().strip('\\n'))\n",
    "\n",
    "    for i in range(ins_test_num):\n",
    "        tmp = list(map(int, fin.readline().strip('\\n').split(' ')))\n",
    "        ins_wrong.append((tmp[0], tmp[1]))\n",
    "        tmp = list(map(int, fin_right.readline().strip('\\n').split(' ')))\n",
    "        ins_right.append((tmp[0], tmp[1]))\n",
    "\n",
    "    fin.close()\n",
    "    fin_right.close()\n",
    "\n",
    "    if valid == 'valid':\n",
    "        if mix:\n",
    "            fin = open(\"../../data/\" + dataSet + \"/M-Valid/subClassOf2id_negative.txt\", 'r', encoding='utf-8')\n",
    "            fin_right = open(\"../../data/\" + dataSet + \"/M-Valid/subClassOf2id_positive.txt\", 'r', encoding='utf-8')\n",
    "        else:\n",
    "            fin = open(\"../../data/\" + dataSet + \"/Valid/subClassOf2id_negative.txt\", 'r', encoding='utf-8')\n",
    "            fin_right = open(\"../../data/\" + dataSet + \"/Valid/subClassOf2id_positive.txt\", 'r', encoding='utf-8')\n",
    "    else:\n",
    "        if mix:\n",
    "            fin = open(\"../../data/\" + dataSet + \"/M-Test/subClassOf2id_negative.txt\", 'r', encoding='utf-8')\n",
    "            fin_right = open(\"../../data/\" + dataSet + \"/M-Test/subClassOf2id_positive.txt\", 'r', encoding='utf-8')\n",
    "        else:\n",
    "            fin = open(\"../../data/\" + dataSet + \"/Test/subClassOf2id_negative.txt\", 'r', encoding='utf-8')\n",
    "            fin_right = open(\"../../data/\" + dataSet + \"/Test/subClassOf2id_positive.txt\", 'r', encoding='utf-8')\n",
    "\n",
    "    sub_test_num = int(fin.readline().strip('\\n'))\n",
    "    sub_test_num = int(fin_right.readline().strip('\\n'))\n",
    "\n",
    "    for i in range(sub_test_num):\n",
    "        tmp = list(map(int, fin.readline().strip('\\n').split(' ')))\n",
    "        sub_wrong.append((tmp[0], tmp[1]))\n",
    "        tmp = list(map(int, fin_right.readline().strip('\\n').split(' ')))\n",
    "        sub_right.append((tmp[0], tmp[1]))\n",
    "\n",
    "    fin.close()\n",
    "    fin_right.close()\n",
    "\n",
    "    # with open(\"../data/\" + dataSet + \"/Train/instance2id.txt\", 'r', encoding='utf-8') as f:\n",
    "    #     entity_num = int(f.readline().strip('\\n'))\n",
    "    # with open(\"../data/\" + dataSet + \"/Train/concept2id.txt\", 'r', encoding='utf-8') as f:\n",
    "    #     concept_num = int(f.readline().strip('\\n'))\n",
    "\n",
    "    # entity_df = pd.read_table(\"../data/\" + dataSet + \"/Train/instance2id.txt\", header=None, skiprows=[0])\n",
    "    # entity_dict = dict(zip(entity_df[0], entity_df[1]))\n",
    "    # entity_num = len(entity_dict)\n",
    "    # concept_df = pd.read_table(\"../data/\" + dataSet + \"/Train/concept2id.txt\", header=None, skiprows=[0])\n",
    "    # concept_dict = dict(zip(concept_df[0], concept_df[1]))\n",
    "    # concept_num = len(concept_dict)\n",
    "\n",
    "    f1 = open(\"../vector/\" + dataSet + \"/entity2vec.vec\", 'r', encoding='utf-8')\n",
    "    f2 = open(\"../vector/\" + dataSet + \"/concept2vec.vec\", 'r', encoding='utf-8')\n",
    "\n",
    "    entity_vec = list()\n",
    "    while True:\n",
    "        line = f1.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        line = line.strip('\\n').split('\\t')[:-1]\n",
    "        line_list = list(map(float, line))\n",
    "        entity_vec.append(line_list)\n",
    "\n",
    "    concept_vec = list()\n",
    "    concept_r = list()\n",
    "    while True:\n",
    "        line_concept = f2.readline().strip('\\n')\n",
    "        line_r = f2.readline().strip('\\n')\n",
    "        if not line_r:\n",
    "            break\n",
    "        line_concept = line_concept.split('\\t')[:-1]\n",
    "        line_concept_list = list(map(float, line_concept))\n",
    "        concept_vec.append(line_concept_list)\n",
    "\n",
    "        line_r = line_r.split('\\t')[:-1]\n",
    "        line_r_list = list(map(float, line_r))\n",
    "        concept_r.append(line_r_list)\n",
    "    \n",
    "\n",
    "    return ins_test_num, sub_test_num, ins_wrong, ins_right, sub_wrong, sub_right, entity_vec, concept_vec, concept_r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Loading instance dict-----\n",
      "#instance: 11084\n",
      "-----Loading concept dict-----\n",
      "#concept: 3708\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''load data dict'''\n",
    "data_dir = '../../data/' + dataSet +'/Train'\n",
    "instance_dict_file = \"instance2id.txt\"\n",
    "relation_dict_file = \"relation2id.txt\"\n",
    "concept_dict_file = \"concept2id.txt\"\n",
    "print('-----Loading instance dict-----')\n",
    "instance_df = pd.read_table(os.path.join(data_dir, instance_dict_file), header=None, skiprows=[0])\n",
    "\n",
    "instance_dict = dict(zip(instance_df[0], instance_df[1])) # name:0\n",
    "instance_dict_0_name = dict(zip(instance_df[1], instance_df[0]))\n",
    "instance_num = len(instance_dict)\n",
    "entities = list(instance_dict.values())\n",
    "print('#instance: {}'.format(instance_num))\n",
    "\n",
    "print('-----Loading concept dict-----')\n",
    "concept_df = pd.read_table(os.path.join(data_dir, concept_dict_file), header=None, skiprows=[0])\n",
    "concept_dict = dict(zip(concept_df[0], concept_df[1]))\n",
    "concept_dict_0_name = dict(zip(concept_df[1], concept_df[0]))\n",
    "concept_num = len(concept_dict)\n",
    "concepts = list(concept_dict.values())\n",
    "print('#concept: {}'.format(concept_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入train文件\n",
    "print('-----Loading instance_of triples-----')\n",
    "instance_of_df = pd.read_table(os.path.join(data_dir, instance_of_file), header=None, sep=' ')\n",
    "instance_of = list(zip(instance_of_df[0], instance_of_df[1]))\n",
    "instance_of_num = len(instance_of)\n",
    "# print('# train instance of :{}'.format(instance_of_num))\n",
    "\n",
    "print('-----Loading subclass_of triples-----')\n",
    "subclass_of_df = pd.read_table(os.path.join(data_dir, subclass_of_file), header=None, sep=' ')\n",
    "subclass_of = list(zip(subclass_of_df[0], subclass_of_df[1]))\n",
    "subclass_of_num = len(subclass_of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.isdir('../info_out'):\n",
    "    os.makedirs('../info_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_dimension(valid,ins_wrong, ins_right, sub_wrong, sub_right,entity_vec, concept_vec, concept_r):\n",
    "    \n",
    "    def check_instance(instance, concept):\n",
    "        dis = np.subtract(concept_r[concept], np.abs(np.subtract(entity_vec[instance], concept_vec[concept])))\n",
    "        dis_temp = np.where(dis > 0, 1, 0)\n",
    "        return dis_temp\n",
    "\n",
    "    def check_subclass(concept1, concept2):\n",
    "        dis = np.subtract(np.subtract(concept_r[concept2], concept_r[concept1]), np.abs(np.subtract(concept_vec[concept1], concept_vec[concept2])))\n",
    "        dis_temp = np.where(dis > 0, 1, 0)\n",
    "        return dis_temp\n",
    "        \n",
    "    ins_right_sum_per_pair = []\n",
    "    ins_right_sum_per_dimension = [0 for i in range(100)]\n",
    "    \n",
    "    with open(os.path.join('../info_out/',valid +'_ins_right.csv'), 'w') as f:\n",
    "        for i in range(len(ins_right)):\n",
    "            dis_temp = check_instance(ins_right[i][0], ins_right[i][1])\n",
    "            ins_right_sum_per_pair.append(np.sum(dis_temp==1))\n",
    "            for j in range(len(dis_temp)):\n",
    "                ins_right_sum_per_dimension[j] += dis_temp[j]\n",
    "            f.write('(' + str(ins_right[i][0]) +'\\t' + str(ins_right[i][1]) + ')' + ',' +\n",
    "                    str(ins_right_sum_per_pair[i]) + ',' + \n",
    "                    '(' + instance_dict_0_name[ins_right[i][0]] + '\\t' + concept_dict_0_name[ins_right[i][1]] +')' + \n",
    "                   '\\n')\n",
    "                      \n",
    "    \n",
    "        \n",
    "    print(ins_right_sum_per_dimension)\n",
    "    ins_wrong_sum_per_pair = []\n",
    "    ins_wrong_sum_per_dimension = [0 for i in range(100)]\n",
    "\n",
    "    with open(os.path.join('../info_out/',valid +'_ins_wrong.csv'), 'w') as f:\n",
    "        for i in range(len(ins_wrong)):\n",
    "            dis_temp = check_instance(ins_wrong[i][0], ins_wrong[i][1])\n",
    "            ins_wrong_sum_per_pair.append(np.sum(dis_temp==1))\n",
    "            for j in range(len(dis_temp)):\n",
    "                ins_wrong_sum_per_dimension[j] += dis_temp[j]\n",
    "            f.write('(' + str(ins_wrong[i][0]) +'\\t' + str(ins_wrong[i][1]) + ')' + ',' +\n",
    "                    str(ins_wrong_sum_per_pair[i]) + ',' + \n",
    "                    '(' + instance_dict_0_name[ins_wrong[i][0]] + '\\t' + concept_dict_0_name[ins_wrong[i][1]] +')' + \n",
    "                   '\\n')\n",
    "                      \n",
    "    \n",
    "    print(ins_wrong_sum_per_dimension)\n",
    "    sub_right_sum_per_pair = []\n",
    "    sub_right_sum_per_dimension = [0 for i in range(100)]\n",
    "\n",
    "    with open(os.path.join('../info_out/',valid +'_sub_right.csv'), 'w') as f:\n",
    "         for i in range(len(sub_right)):\n",
    "            dis_temp = check_subclass(sub_right[i][0], sub_right[i][1])\n",
    "            sub_right_sum_per_pair.append(np.sum(dis_temp==1))\n",
    "            for j in range(len(dis_temp)):\n",
    "                sub_right_sum_per_dimension[j] += dis_temp[j]\n",
    "            f.write('(' + str(sub_right[i][0]) +'\\t' + str(sub_right[i][1]) + ')' + ',' +\n",
    "                    str(sub_right_sum_per_pair[i]) + ',' + \n",
    "                    '(' + concept_dict_0_name[sub_right[i][0]] + '\\t' + concept_dict_0_name[sub_right[i][1]] +')' + \n",
    "                   '\\n')\n",
    "    sub_wrong_sum_per_pair = []\n",
    "    sub_wrong_sum_per_dimension = [0 for i in range(100)]\n",
    "    with open(os.path.join('../info_out/',valid +'_sub_wrong.csv'), 'w') as f:\n",
    "         for i in range(len(sub_wrong)):\n",
    "            dis_temp = check_subclass(sub_wrong[i][0], sub_wrong[i][1])\n",
    "            sub_wrong_sum_per_pair.append(np.sum(dis_temp==1))\n",
    "            for j in range(len(dis_temp)):\n",
    "                sub_wrong_sum_per_dimension[j] += dis_temp[j]\n",
    "            f.write('(' + str(sub_wrong[i][0]) +'\\t' + str(sub_wrong[i][1]) + ')' + ',' +\n",
    "                    str(sub_wrong_sum_per_pair[i]) + ',' + \n",
    "                    '(' + concept_dict_0_name[sub_wrong[i][0]] + '\\t' + concept_dict_0_name[sub_wrong[i][1]] +')' + \n",
    "                   '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----prepare data -----\n",
      "[2195, 2215, 2176, 2206, 2137, 2195, 2148, 2211, 2211, 2142, 2227, 2168, 2159, 2150, 2148, 2192, 2192, 2186, 2179, 2168, 2232, 2123, 2185, 2159, 2177, 2056, 2136, 2112, 2160, 2196, 2116, 2168, 2161, 2193, 2203, 2209, 2152, 2151, 2196, 2156, 2169, 2211, 2208, 2194, 2179, 2190, 2115, 2066, 2168, 2211, 2173, 2183, 2182, 2253, 2183, 2156, 2137, 2197, 2171, 2158, 2222, 2150, 2192, 2162, 2197, 2247, 2136, 2160, 2069, 2189, 2201, 2164, 2083, 2164, 2225, 2148, 2193, 2175, 2227, 2134, 2152, 2072, 2178, 2130, 2134, 2159, 2248, 2172, 2245, 2160, 2165, 2224, 2225, 2222, 2155, 2208, 2201, 2177, 2215, 2205]\n",
      "[1129, 1096, 1193, 1114, 1170, 1081, 1126, 1108, 1097, 1143, 1079, 1125, 1171, 1146, 1134, 1173, 1101, 1113, 1102, 1178, 1094, 1209, 1143, 1154, 1135, 1241, 1131, 1197, 1164, 1118, 1190, 1125, 1166, 1038, 1126, 1106, 1112, 1099, 1119, 1154, 1081, 1051, 1122, 1158, 1103, 1100, 1211, 1147, 1154, 1168, 1123, 1125, 1123, 1049, 1148, 1155, 1127, 1178, 1157, 1143, 1032, 1091, 1078, 1104, 1072, 1113, 1117, 1156, 1208, 1200, 1107, 1153, 1191, 1250, 1142, 1214, 1114, 1134, 1061, 1195, 1204, 1250, 1097, 1132, 1163, 1140, 1129, 1192, 1152, 1202, 1101, 1169, 1057, 1163, 1213, 1082, 1162, 1107, 1070, 1134]\n",
      "-----prepare data -----\n",
      "[4658, 4696, 4562, 4628, 4583, 4616, 4597, 4657, 4632, 4578, 4699, 4616, 4520, 4564, 4613, 4610, 4603, 4617, 4619, 4594, 4668, 4486, 4611, 4563, 4571, 4379, 4601, 4486, 4591, 4624, 4505, 4627, 4589, 4628, 4612, 4666, 4561, 4580, 4664, 4603, 4569, 4693, 4653, 4611, 4620, 4645, 4482, 4472, 4590, 4715, 4606, 4622, 4647, 4739, 4621, 4531, 4564, 4649, 4575, 4583, 4680, 4615, 4703, 4652, 4651, 4745, 4526, 4572, 4410, 4586, 4643, 4548, 4458, 4595, 4687, 4566, 4633, 4589, 4660, 4505, 4564, 4412, 4581, 4599, 4529, 4583, 4706, 4535, 4656, 4587, 4597, 4647, 4673, 4605, 4512, 4687, 4622, 4570, 4740, 4667]\n",
      "[2237, 2174, 2365, 2239, 2351, 2216, 2247, 2207, 2232, 2253, 2167, 2283, 2339, 2262, 2328, 2378, 2235, 2244, 2237, 2355, 2205, 2394, 2295, 2309, 2298, 2508, 2304, 2371, 2252, 2253, 2366, 2231, 2314, 2091, 2255, 2210, 2239, 2200, 2245, 2316, 2202, 2172, 2233, 2291, 2222, 2206, 2450, 2328, 2305, 2321, 2233, 2314, 2265, 2091, 2313, 2345, 2280, 2401, 2327, 2280, 2159, 2152, 2166, 2214, 2157, 2262, 2259, 2301, 2441, 2369, 2243, 2315, 2434, 2444, 2269, 2425, 2256, 2236, 2150, 2393, 2487, 2473, 2210, 2277, 2360, 2311, 2254, 2371, 2317, 2353, 2210, 2302, 2116, 2312, 2472, 2166, 2291, 2213, 2217, 2260]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ins_wrong, ins_right, sub_wrong, sub_right = list(), list(), list(), list()\n",
    "# 测试 test set\n",
    "valid = 'test'\n",
    "ins_test_num, sub_test_num, ins_wrong, ins_right, sub_wrong, sub_right, entity_vec, concept_vec, concept_r \\\n",
    "    = prepare(valid, mix, dataSet, ins_wrong, ins_right, sub_wrong, sub_right)\n",
    "sum_dimension(valid,ins_wrong, ins_right, sub_wrong, sub_right, entity_vec, concept_vec, concept_r)\n",
    "# 测试valid set\n",
    "valid = 'valid'\n",
    "ins_test_num, sub_test_num, ins_wrong, ins_right, sub_wrong, sub_right, entity_vec, concept_vec, concept_r \\\n",
    "    = prepare(valid, mix, dataSet, ins_wrong, ins_right, sub_wrong, sub_right)\n",
    "sum_dimension(valid, ins_wrong, ins_right, sub_wrong, sub_right, entity_vec, concept_vec, concept_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([[[1,2,3,2],[1,2,3,1],[2,3,4,1]],[[1,0,2,0],[2,1,2,0],[2,1,1,1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "b = np.sum(a == 1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "统计train、valid的，然后详细输入文件之中"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
